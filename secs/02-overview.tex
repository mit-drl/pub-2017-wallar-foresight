% !TEX root = ../foresight.tex

\subsection{Method overview}

We consider two vehicles.
\begin{itemize}
\item
A ground vehicle, i.e. the car, which can create a local map of the environment, localize with respect to it and autonomously navigate \cite{Add citation to Felix's IV paper}. We utilize a 2D grid map to represent the free space and obstacles seen by the car. In particular, our vehicle is equipped with a 2D LIDAR. 
\item
A lightweight companion quadrotor \footnote{We use a Parrot Bebop 2} equipped with a front facing camera. The drone is able to fly autonomously to/from the ground vehicle and detect obstacles that were originally occluded for the ground vehicle.
\end{itemize}

Given a laser scan from the ground vehicle, our objective is to:
a) determine which areas of the environment are occluded to ground vehicle,
b) compute a safe path for the aerial vehicle to observe the occluded areas, and
c) detect unseen obstacles, such as pedestrians, and report them back to the ground vehicle.

% Our problem is two pronged. First we need to be able to accurately localize the
% quadrotor in the same reference frame as the vehicle. Second, we need to
% determine which areas of the environment are occluded to ground vehicle, and
% compute a safe path that leads the quadrotor to view these regions.

% % \begin{problem}
% %
% %     Given a laser scan from the ground vehicle, compute a path the maximizes
% %     the occluded area observed by the quadrotor for a given time horizon.
% % \end{problem}
%
% We used a Parrot Bebop 2 as the quadcopter. It is equipped with a camera, and
% velocity and altitude measurements can be read from its SDK at a rate of 5 Hz.
% The Bebop uses a software gimbal to stabilize the camera image.
%
% The inputs to the Bebop are roll angle $\phi$, pitch angle $\theta$, 
% yaw rate $\omega$, and z velocity $\dot{z}$. We use
% PID controllers to control the inputs.
%
% $$ u = [\phi, \theta, \omega, \dot{z}] $$

%\subsection{Method overview}

To accurately localize the quadrotor relative to the ground vehicle, we equip the ground vehicle with several Ultra-wideband radios. In the quadrotor, we fuse, via an Unscented Kalman filter (UKF) relative information from a UWB radio with odometry estimates from a down facing optical flow sensor and an onboard IMU.
% We use an unscented Kalman filter to fuse these
% measurements and produce an estimate of quads 3D position relative to the
% ground vehicle.
Our algorithm operates directly on the laser scan from the ground vehicle to find occluded
regions.
We then employ an anytime sampling-based algorithm to compute a collision free path for the drone
that maximizes the occluded area viewed by the quadrotor.
% and builds a search tree from the quadrotor's current configuration
% towards these regions. The search terminates when a timeout has expired and
% returns the path that currently maximizes the occluded area viewed by the
% quadrotor.
To detect obstacles within the occluded areas, we employ a real-time object
detecting convolutional neural network~\cite{redmond2016you}, which is able to
classify and locate objects, such as pedestrians, cars, bicycles, in monocular
images.  These obstacles are then reported back to the ground vehicle.



Thanks to the differential flatness of quadrotor vehicles, we model our quadrotor by its position $\bm{p}$ and yaw orientation $\psi$, measured in the car's body frame. We therefore describe the state of the quadrotor as

$$
    x = [\bm{p}, \psi] \in \R^4
$$

Using the ground vehicle's 2D laser scan, we compute the areas it is unable to
sense. We employ an anytime sampling based algorithm to construct a collision
free path for the quadrotor that maximizes the total area of the occluded
regions it is able to observe. While the quadrotor is executing the planned
path, we use a convolutional neural network to classify and detect objects in
the quadrotor's field of view, such as pedestrians and cars, and relay this
information back to the ground vehicle. The path is updated if it is no longer
collision free due to changes in the laser scan or if a new path is computed
that can observe a larger occluded area. On high level overview of the entire
process is shown in Algo.~\ref{algo:overview}.

\begin{algorithm}[h!]
    \caption{Overview of the Foresight algorithm}
    \label{algo:overview}
    \begin{algorithmic}[1]
        \setcounter{ALC@line}{0}
        \STATE $\Pi \leftarrow \emptyset$
        \WHILE {$\Function{IsRunning}()$}
        \STATE $x \leftarrow \Function{GetQuadrotorConfiguration}()$
        \STATE $L \leftarrow \Function{GetLaserScan}()$
        \STATE $\Poly \leftarrow \Function{ConstructBoundingPolygon}(L)$
        \STATE $\B \leftarrow \Function{ComputeBlindRegions}(L)$
        \STATE $\widetilde{\Pi} \leftarrow
        \Function{ComputeCoveragePath}(x, \B, \Poly)$
        \IF {$|\Pi| = 0 \vee \neg \Function{PathCollisionFree}(\Pi, \Poly)
            \vee$ \\ $\pushcode \Function{ObservingArea}(\widetilde{\Pi}, \B) >
            \Function{ObservingArea}(\Pi, \B)$}
            \STATE $\Pi \leftarrow \widetilde{\Pi}$
        \ENDIF
        \STATE $\Function{SendPathToQuadrotor}(\Pi)$
        \ENDWHILE
    \end{algorithmic}
\end{algorithm}

%
% \ja{Can you add here a high-level algorithm? Car maps -> compute blind spots -> plan path -> navigate to bind spots -> detect occluded obstacles -> loop}
